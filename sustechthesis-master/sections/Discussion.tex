\section{Discussion}
\subsection{Performance of ZIVA}
Dimensionality reduction is an essential step for downstream analysis such as clustering and trajectory inference. In this study, we proposed a new model for reducing the dimensionality of single cell sequencing data: zero-inflated variational autoencoder (ZIVA) based on the model in VASC. We tested the performance of ZIVA on several datasets and compared it with six other methods. The result shows that ZIVA can achieve better results in some cases especially for lineage data. 

Some popular methods such as tSNE focus on the local structure but cannot preserve global structure well. It can greatly separate the clusters of the cells but may fails when data are continuous distributed. Some methods try to keep global structure by maximize the explainable variations but fail to preserve subtle local structures such as PCA. ZIVA have a good balance between local structure and global structure. The result shows that ZIVA can recover the subtle batch effects and can also well recover the cell developmental lineages.

Compared with tSNE and UMAP, it is easier to add a new data point to the visualization of ZIVA because it doesn't need to re-run the algorithm with the whole dataset. A shortcoming of ZIVA is the relatively long running time. For the large datasets that contains thousands of cells, ZIVA may cost several hours on a desktop-level computer. It would be better if we can compress the model to reduce the time costing. Also, for the large datasets, some pre-transformation like feature selection of input matrix is necessary. For example, Seurat select features by highly variable genes \cite{Satija2015}. M3drop \cite{andrews2019m3drop} performs feature selection based on the dropout rate and removes the genes that only affected by technical noises.

Our model is developed based on Variational autoencoder (VAE) and VASC. Compared with VASC, we did more improvments on VAE and get higher performance on most datasets. Firstly, our model is deeper (4 layers) compared with the model of VASC (3 layers). Many research has shown that the deeper network has higher ability of representing complex non-linear functions. We added an optional dropout model (MM model) for modeling the probability of dropout events in scRNA-seq data. The model parameter in VASC is fixed to 1. However, the dropout distribution may vary in different datasets. So, we added the process of parameter fitting in our model. Also, we used mean square error (MSE) as the loss function. Compared with the cross-entropy loss in VASC, MSE is more suitable in regression problems. 

However, some attempts fails. For example, we tried to add nuclear norm to the matrix outputed from decoder but didn't get good result. I have analyzed the reason in the analysis part. We also tried to add nuclear norm to the latent variables, but also failed. Moreover, we tried to use a neuron network to better model the dropout function. However, to realize that, we need to give the matrix from decoder a powerful constraint to ensure that matrix is close to the true expression matrix. But we didn't find a proper constraint and the network fails to learn the true expression matrix. We also tried to build the dynamics of the gene expression such as in RNA velocity \cite{la2018rna}. If we can define the dynamics of single cell expression development, we can use Koopman theory \cite{morton2018deep} and the deep autoencoder structure to recover the dynamics in two dimensionality \cite{lusch2018deep} and thus reduce the dimensionality of the data and show the dynamics of the data points at the same time. however, we failed to find a universal dynamics in the single cell expression data.  

Variational autoencoder is a great framework with high expansibility. Except from optimizing ZIVA model, we are also trying some other possible applications. The first is the clustering friendly autoencoder. By simultaneously performing dimensionality reduction and clustering, it is possible to get better performance on both tasks. The second is the graph variational autoencoder model. It performs the dimensionality reduction by constructing the graph structure in high dimensional space and preserving it to two dimensions. Also, we can try using generative adversarial networks to measure the difference of two data distributions. It may have better results than simple mean square error. These works are still in progress.

\subsection{Future works}

\subsubsection{Clustering friendly dimensionality reduction}
Because of the high dimensionality and high sparsity of the single cell sequencing data, conventional clustering methods usually have poor performance on high dimensional data. Therefore, scRNA-seq data needs to be presented in a lower dimensional space by reducing the dimensionality to enable the clustering process. The traditional pipelines usually perform dimensionality reduction and clustering as separate parts \cite{wu2020tools}. However, some researches recently shows that performing dimensionality reduction and clustering jointly can achieve better results of both clustering and dimensionality reduction \cite{yang2017towards}. For example, some researchers use Gussian mixture model to cluster images when performing variational autoencoder to reduce the dimensionality of those images \cite{prasad2020variational}. Such methods that uses deep learning to aid clustering tasks are called deep clustering.

For scRNA-seq data, we can also perform deep clustering to improve the performance of clustering and dimensionality reduction. Because there are usually several cell types in a scRNA-seq dataset, in the latent space, those cell types should fall into clusters naturally. For scRNA-seq analysis, we hope to distinguish those clusters by dimensionality reduction and clustering. In this case, we can perform deep clustering methods to cluster cells and reduce the dimensionality of data at the same time. By doing so, we can not only get nice clustering results but also get a great visualization result that have greatly separated clusters that enable us to identify cell sub-populations clearly \cite{tian2019clustering}. 

We performed deep clustering by applying the clustering algorithms to the two dimensional latent variables of variational autoencoder. The basic structure are as Figure \ref{dcstru}. 

\begin{figure}[htb!]
    \centering
    \includegraphics[width=1\textwidth]{figures/myfigures/dc.png}
    \caption{The structure of the deep clustering model}
    \label{dcstru}
\end{figure}

For the clustering process, we tried four methods: the spectral clustering \cite{von2007tutorial}, DBSCAN \cite{ester1996density}, K-means \cite{kanungo2002efficient} and louvain methods \cite{traag2019louvain}. The louvain method can achieve better visualization performance than other three methods. However, the overall result is not good enough now. We are still trying to optimize the model.

It is also worth trying to jointly reduce the dimensionality and infer the cell lineage. Unlike performing clustering tasks in latent space in deep clustering methods, we can constrain the latent variables to form a tree structure or a graph structure and thus get the latent variables that is friendly for trajectory inference.

The optimization of this kind of models consists two parts. Firstly, pre-train the network as a normal variational autoencoder. Then, performing alternating trainging to optimize the clustering loss and reconstruction loss at the same time.

\subsubsection{GAN for scRNA-seq data}
Generative Adversarial Networks \cite{radford2015unsupervised} are extensively used for unsupervised representation learning in recent years. In single cell data analysis domain, it has been used for dimensionality reduction \cite{lin2020deep}, data generation and data augmentation \cite{marouf2018realistic}. For dimensionality reduction, the structure in figure can be used. In this model, the loss function in variational autoencoder is replaced by a discriminator which is a multi-layer neuron network with decreasing unit layers. Because of the dropout events, using pair-wise distance to measure the differnece between two expression matrix can be inaccurate. In this way, the discriminator can learn more complex functions to distinguish the true expression and generated expression than the simple mean squared error function. Moreover, the discriminator network can also be combined with other models such as clustering friendly dimensionality reduction model and the ZIVA model to generate more powerful methods.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=1\textwidth]{figures/myfigures/vaegan.png}
    \caption{The structure of the VAE-GAN model}
    \label{dcstru}
\end{figure}

\subsubsection{GVAE for scRNA-seq data}
Recent years, graph neuron network has been widely used in various industries such as social network \cite{yang2016revisiting}, recommendation systems \cite{kipf2016semi}. Single-cell sequencing data can also be viewed as a graph where each node is a cell. For example, a study \cite{ravindra2020disease} uses graph convolutional neuron network \cite{velivckovic2017graph} to predict the disease state of Multiple Sclerosis. The graph structure can also be applied to the variational autoencoder model \cite{kipf2016variational}. It is possible that though the preservation of graph structure of single cells in high dimensionality spaces, graph variational autoencoder can have better dimensionality reduction performance on global cell structures.

